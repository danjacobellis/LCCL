{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae16cc7-9519-4223-8ba2-3f65f134600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import warnings\n",
    "import IPython.display\n",
    "import io\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from datasets import load_dataset\n",
    "from torchvision.transforms import (\n",
    "    RandomResizedCrop, Resize, CenterCrop,\n",
    "    PILToTensor, ToPILImage, \n",
    "    Compose, RandomHorizontalFlip )\n",
    "from vit_pytorch.simple_vit_with_register_tokens import SimpleViT\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "class Config: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b2aae6-9b16-4dc2-86ef-79d5a6093fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:3\"\n",
    "\n",
    "config = Config()\n",
    "# Training and optimizer config\n",
    "config.batch_size = 128\n",
    "config.grad_accum_steps = 1\n",
    "config.min_lr = 1e-6\n",
    "config.max_lr = 4e-4\n",
    "config.warmup_steps = 50000\n",
    "config.plot_update = 128\n",
    "config.patience = 128\n",
    "config.weight_decay = 0.\n",
    "config.epochs = 50\n",
    "config.num_workers = 12\n",
    "config.valid_image_size = 288\n",
    "\n",
    "# Classification model config\n",
    "config.image_size = 256\n",
    "config.channels = 3\n",
    "config.patch_size = 16\n",
    "config.num_classes = 1000\n",
    "config.embed_dim = 96\n",
    "config.depth = 12\n",
    "config.heads = 3\n",
    "config.mlp_dim = 768\n",
    "config.dim_head = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c35459-ecdb-4933-a741-eb188f573596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.842312"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleViT(\n",
    "    image_size = config.image_size,\n",
    "    channels = config.channels,\n",
    "    patch_size = config.patch_size,\n",
    "    num_classes = config.num_classes,\n",
    "    dim = config.embed_dim,\n",
    "    depth = config.depth,\n",
    "    heads = config.heads,\n",
    "    mlp_dim = config.mlp_dim,\n",
    "    dim_head = config.dim_head\n",
    ").to(device)\n",
    "sum(p.numel() for p in model.parameters())/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b87f73a-8824-434a-824f-ba90057bc338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a310c48a9240d49f9f722d3e4b1148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_train = load_dataset('imagenet-1k',split='train',trust_remote_code=True)\n",
    "dataset_valid = load_dataset('imagenet-1k',split='validation',trust_remote_code=True)\n",
    "dataset_valid2 = load_dataset('danjacobellis/imagenet-a',split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6767f4d3-3df3-4a67-a9a5-bf71998f8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = config.image_size\n",
    "C = config.channels\n",
    "\n",
    "train_transform = Compose([\n",
    "    RandomResizedCrop(\n",
    "        size=(L,L),\n",
    "        interpolation=Image.Resampling.LANCZOS\n",
    "    ),\n",
    "    RandomHorizontalFlip(0.5),\n",
    "    PILToTensor()\n",
    "])\n",
    "\n",
    "valid_transform = Compose([\n",
    "    Resize(\n",
    "        size=config.valid_image_size,\n",
    "        interpolation=Image.Resampling.LANCZOS\n",
    "    ),\n",
    "    CenterCrop(size=L),\n",
    "    PILToTensor(),\n",
    "])\n",
    "\n",
    "def train_collate_fn(batch):\n",
    "    B = len(batch)    \n",
    "    x = torch.zeros( (B, C, L, L), dtype=torch.uint8)\n",
    "    y = torch.zeros(B, dtype=torch.int)\n",
    "    for i_sample, sample in enumerate(batch):\n",
    "        img = sample['image'].convert(\"RGB\")\n",
    "        x[i_sample,:,:,:] = train_transform(img)\n",
    "        y[i_sample] = sample['label']\n",
    "    return x, y\n",
    "\n",
    "def valid_collate_fn(batch):\n",
    "    B = len(batch)    \n",
    "    x = torch.zeros( (B, C, L, L), dtype=torch.uint8)\n",
    "    y = torch.zeros(B, dtype=torch.int)\n",
    "    for i_sample, sample in enumerate(batch):\n",
    "        img = sample['image'].convert(\"RGB\")\n",
    "        x[i_sample,:,:,:] = valid_transform(img)\n",
    "        y[i_sample] = sample['label']\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cedac284-9bce-46a1-948a-36693e204d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=config.min_lr,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "def minus_cosine_warmup(i_step):\n",
    "    scale = 0.5 * (np.log10(config.max_lr) - np.log10(config.min_lr))\n",
    "    angle =  np.pi * i_step / (config.warmup_steps//config.plot_update)\n",
    "    log_lr = np.log10(config.min_lr) + scale * (1 - np.cos(angle))\n",
    "    lr = 10 ** log_lr\n",
    "    return lr/config.min_lr\n",
    "    \n",
    "warmup = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda = lambda i_step: minus_cosine_warmup(i_step)\n",
    ")\n",
    "\n",
    "reduce_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    factor=0.98,\n",
    "    patience=config.patience,\n",
    "    threshold=1e-3,\n",
    "    min_lr=config.min_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f7abb-cb89-49fe-9438-8eb76fedda14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/50 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='127' class='' max='10009' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.27% [127/10009 00:15&lt;19:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\"Corrupt EXIF data.  Expecting to read .* bytes but only got .*\", category=UserWarning, module=\"PIL.TiffImagePlugin\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Metadata Warning, tag .* had too many entries: .* expected .*\", category=UserWarning, module=\"PIL.TiffImagePlugin\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Truncated File Read\", category=UserWarning, module=\"PIL.TiffImagePlugin\")\n",
    "\n",
    "mb = master_bar(range(config.epochs))\n",
    "mb.names = ['per batch','smoothed']\n",
    "train_loss = []\n",
    "valid_acc = []\n",
    "valid_acc_a = []\n",
    "learning_rates = [optimizer.param_groups[0]['lr']]\n",
    "i_step = 0\n",
    "for i_epoch in mb:\n",
    "    # training\n",
    "    model.train()\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers,\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "        collate_fn=train_collate_fn\n",
    "    )\n",
    "    pb = progress_bar(dataloader_train, parent=mb)\n",
    "    for i_batch, (x,y) in enumerate(pb):\n",
    "       \n",
    "        x = x.to(torch.float)\n",
    "        x = x/255\n",
    "        x = x.to(device)\n",
    "        y = y.to(torch.long).to(device)\n",
    "        \n",
    "        # forward\n",
    "        logits = model(x)\n",
    "        loss = nn.CrossEntropyLoss()(input=logits, target=y)\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        if (i_batch + 1) % config.grad_accum_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # plotting\n",
    "        if (i_step+1) % config.plot_update == 0:\n",
    "            plot_n = len(train_loss) // config.plot_update\n",
    "            smoothed_x = (0.5+torch.arange(plot_n)) * config.plot_update\n",
    "            smoothed_y = torch.tensor(train_loss).reshape(plot_n, -1).mean(dim=1)\n",
    "            train_x = range(len(train_loss))\n",
    "            train_y = train_loss\n",
    "            mb.update_graph([[train_x, np.log10(train_y)],[smoothed_x, np.log10(smoothed_y)]])\n",
    "\n",
    "            # lr scheduler\n",
    "            if i_step < config.warmup_steps:\n",
    "                warmup.step()\n",
    "            else:\n",
    "                reduce_plateau.step(smoothed_y[-1])\n",
    "            learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "        i_step+=1\n",
    "        \n",
    "    # validation\n",
    "    model.eval()\n",
    "    valid_acc.append(0)\n",
    "    valid_acc_a.append(0)\n",
    "    data_loader_valid = torch.utils.data.DataLoader(\n",
    "        dataset_valid,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        collate_fn=valid_collate_fn\n",
    "    )\n",
    "    \n",
    "    data_loader_valid2 = torch.utils.data.DataLoader(\n",
    "        dataset_valid2,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        collate_fn=valid_collate_fn\n",
    "    )\n",
    "    \n",
    "    # standard validation set\n",
    "    pb_valid = progress_bar(data_loader_valid, parent=mb)\n",
    "    for i_batch, (x, y) in enumerate(pb_valid):\n",
    "        x = x.to(torch.float) / 255\n",
    "        x = x.to(device)\n",
    "        y = y.to(torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "        valid_acc[i_epoch] += torch.sum(logits.argmax(dim=1) == y).item()\n",
    "    valid_acc[i_epoch] /= len(dataset_valid)\n",
    "    \n",
    "    # adversarial validation set\n",
    "    pb_valid2 = progress_bar(data_loader_valid2, parent=mb)\n",
    "    for i_batch, (x, y) in enumerate(pb_valid2):\n",
    "        x = x.to(torch.float) / 255\n",
    "        x = x.to(device)\n",
    "        y = y.to(torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "        valid_acc_a[i_epoch] += torch.sum(logits.argmax(dim=1) == y).item()\n",
    "    valid_acc_a[i_epoch] /= len(dataset_valid2)\n",
    "    \n",
    "    # Update the progress bar comment\n",
    "    mb.main_bar.comment = f'valid_acc {valid_acc[i_epoch]:.4g}, valid_acc_a {valid_acc_a[i_epoch]:.4g}'\n",
    "    \n",
    "    # Modify the torch.save call\n",
    "    torch.save({\n",
    "        'i_epoch': i_epoch,\n",
    "        'learning_rates': learning_rates,\n",
    "        'smoothed_y': smoothed_y,\n",
    "        'valid_acc': valid_acc,\n",
    "        'valid_acc_a': valid_acc_a,\n",
    "        'config': config\n",
    "    }, f\"log_{device}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d282cb-14ed-4aea-999f-a51fa58e3ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, update_display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "display(HTML(mb.main_bar.progress))\n",
    "display(HTML(mb.child.progress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a84fc29-fa05-4780-89b2-d2fd3f8bc89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_train_loss = 10 * (7 - smoothed_y)\n",
    "valid_acc_pct = 100*torch.tensor(valid_acc)\n",
    "fig, ax1 = plt.subplots()\n",
    "line1, = ax1.plot(fn_train_loss, label='Scaled (7 - Smoothed Y)', color='black')\n",
    "ax1.set_xlabel('Steps')\n",
    "ax1.tick_params(axis='x', colors='black')\n",
    "ax1.tick_params(axis='y', colors='black')\n",
    "ax2 = ax1.twiny()\n",
    "line2, = ax2.plot(valid_acc_pct, label='Validation Accuracy', color='blue')\n",
    "ax2.set_xlabel('Epochs', color='blue')  # Label for the top x-axis\n",
    "ax2.tick_params(axis='x', colors='blue')\n",
    "ax2.tick_params(axis='y', colors='blue')\n",
    "ax1.legend(handles=[line1, line2], labels=['∝(7 - training loss) [Nats]', 'Validation Accuracy [%]'], loc='lower right')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ce8d1-f8dc-4677-ae30-3dccdfa189d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f031575-abe3-46cb-9dca-5e0bd998676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_acc[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
