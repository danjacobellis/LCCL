{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3437a1-a1de-415d-90ab-55dc5de17b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import warnings\n",
    "import IPython.display\n",
    "import io\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from datasets import load_dataset\n",
    "from torchvision.transforms import (\n",
    "    RandomResizedCrop, Resize, Grayscale,\n",
    "    PILToTensor, ToPILImage, \n",
    "    Compose, RandomHorizontalFlip )\n",
    "from DenseViT import DenseViT\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "class Config: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120541eb-bdb2-4e19-b9f9-2d17825e2f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:3\"\n",
    "\n",
    "config = Config()\n",
    "# Training and optimizer config\n",
    "config.batch_size = 128\n",
    "config.grad_accum_steps = 1\n",
    "config.min_lr = 1e-6\n",
    "config.max_lr = 1e-4 \n",
    "config.warmup_steps = 10000\n",
    "config.plot_update = 128\n",
    "config.patience = 128\n",
    "config.weight_decay = 0.\n",
    "config.epochs = 30\n",
    "config.num_workers = 12\n",
    "config.valid_samples_per_epoch = 256\n",
    "\n",
    "# Colorization model config\n",
    "config.image_size = 128\n",
    "config.channels = 3\n",
    "config.patch_size = 8\n",
    "config.num_classes = 192\n",
    "config.embed_dim = 192\n",
    "config.depth = 12\n",
    "config.heads = 3\n",
    "config.mlp_dim = 768\n",
    "config.dim_head = config.embed_dim//config.heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379149c-8c1a-4071-a451-0220f00afa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseViT(\n",
    "    image_size = config.image_size,\n",
    "    channels = config.channels,\n",
    "    patch_size = config.patch_size,\n",
    "    num_classes = config.num_classes,\n",
    "    dim = config.embed_dim,\n",
    "    depth = config.depth,\n",
    "    heads = config.heads,\n",
    "    mlp_dim = config.mlp_dim,\n",
    "    dim_head = config.dim_head\n",
    ").to(device)\n",
    "sum(p.numel() for p in model.parameters())/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7519c0e5-fcbe-4fa5-9bb7-9b5dc6fcd98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = load_dataset('imagenet-1k',split='train',trust_remote_code=True)\n",
    "dataset_valid = load_dataset('imagenet-1k',split='validation',trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e3238-f70e-4808-965d-3cf00e51d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = config.image_size\n",
    "C = config.channels\n",
    "\n",
    "train_transform = Compose([\n",
    "    RandomResizedCrop(\n",
    "        size=(L,L),\n",
    "        scale=(0.5, 1),\n",
    "        interpolation=Image.Resampling.LANCZOS\n",
    "    ),\n",
    "    RandomHorizontalFlip(0.5),\n",
    "    PILToTensor()\n",
    "])\n",
    "\n",
    "valid_transform = Compose([\n",
    "    Resize(\n",
    "        size=(L,L),\n",
    "        interpolation=Image.Resampling.LANCZOS\n",
    "    ),\n",
    "    PILToTensor(),\n",
    "])\n",
    "\n",
    "def train_collate_fn(batch):\n",
    "    B = len(batch)    \n",
    "    x = torch.zeros( (B, C, L, L), dtype=torch.uint8)\n",
    "    y = torch.zeros( (B, C, L, L), dtype=torch.uint8)\n",
    "\n",
    "    for i_sample, sample in enumerate(batch):\n",
    "        img = sample['image'].convert(\"RGB\")\n",
    "        y[i_sample,:,:,:] = train_transform(img)\n",
    "        x[i_sample,:,:,:] = Grayscale(num_output_channels=3)(y[i_sample,:,:,:])\n",
    "    return x, y\n",
    "\n",
    "def valid_collate_fn(batch):\n",
    "    B = len(batch)    \n",
    "    x = torch.zeros( (B, C, L, L), dtype=torch.uint8)\n",
    "    y = []\n",
    "    for i_sample, sample in enumerate(batch):\n",
    "        img = sample['image'].convert(\"RGB\")\n",
    "        x[i_sample,:,:,:] = Grayscale(num_output_channels=3)(valid_transform(img))\n",
    "        y.append(img)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd00d2bc-09cd-4b61-8f68-095cd4786de3",
   "metadata": {},
   "outputs": [],
   "source": [
    " display_samples = torch.utils.data.DataLoader(\n",
    "    dataset_valid.select([2,36,46,83]),\n",
    "    batch_size=4,\n",
    "    num_workers=12,\n",
    "    drop_last=True,\n",
    "    shuffle=False,\n",
    "    collate_fn=valid_collate_fn\n",
    ")\n",
    "valid_x, valid_y = next(iter(display_samples))\n",
    "valid_x = valid_x.to(torch.float)\n",
    "valid_x = valid_x / 255\n",
    "valid_x = valid_x.to(device)\n",
    "for vy in valid_y:\n",
    "    display(vy.resize((384,384)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa20de7-1743-48d2-b2c7-6d3f9d372fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=config.min_lr,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "def minus_cosine_warmup(i_step):\n",
    "    scale = 0.5 * (np.log10(config.max_lr) - np.log10(config.min_lr))\n",
    "    angle =  np.pi * i_step / (config.warmup_steps//config.plot_update)\n",
    "    log_lr = np.log10(config.min_lr) + scale * (1 - np.cos(angle))\n",
    "    lr = 10 ** log_lr\n",
    "    return lr/config.min_lr\n",
    "    \n",
    "warmup = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda = lambda i_step: minus_cosine_warmup(i_step)\n",
    ")\n",
    "\n",
    "reduce_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    factor=0.98,\n",
    "    patience=config.patience,\n",
    "    threshold=1e-3,\n",
    "    min_lr=config.min_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f983dfb0-b42b-4469-b983-415b01d4751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\"Corrupt EXIF data.  Expecting to read .* bytes but only got .*\", category=UserWarning, module=\"PIL.TiffImagePlugin\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Metadata Warning, tag .* had too many entries: .* expected .*\", category=UserWarning, module=\"PIL.TiffImagePlugin\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Truncated File Read\", category=UserWarning, module=\"PIL.TiffImagePlugin\")\n",
    "\n",
    "mb = master_bar(range(config.epochs))\n",
    "mb.names = ['per batch','smoothed']\n",
    "img_displays = []\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "learning_rates = [optimizer.param_groups[0]['lr']]\n",
    "\n",
    "i_step = 0\n",
    "for i_epoch in mb:\n",
    "    # training\n",
    "    model.train()\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers,\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "        collate_fn=train_collate_fn\n",
    "    )\n",
    "    pb = progress_bar(dataloader_train, parent=mb)\n",
    "    for i_batch, (x,y) in enumerate(pb):\n",
    "        x = x.to(torch.float)\n",
    "        y = y.to(torch.float)\n",
    "        x = x / 255\n",
    "        y = y / 255\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        loss = nn.functional.mse_loss(model(x), y)\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        if (i_batch + 1) % config.grad_accum_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # plotting\n",
    "        if len(train_loss) % config.plot_update == 0:\n",
    "            plot_n = len(train_loss) // config.plot_update\n",
    "            smoothed_x = (0.5+torch.arange(plot_n)) * config.plot_update\n",
    "            smoothed_y = torch.tensor(train_loss).reshape(plot_n, -1).mean(dim=1)\n",
    "            train_x = range(len(train_loss))\n",
    "            train_y = train_loss\n",
    "            mb.update_graph([[train_x, np.log10(train_y)],[smoothed_x, np.log10(smoothed_y)]])\n",
    "\n",
    "            # lr scheduler\n",
    "            if i_step < config.warmup_steps:\n",
    "                warmup.step()\n",
    "            else:\n",
    "                reduce_plateau.step(smoothed_y[-1])\n",
    "            learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            mb.child.comment = f'loss {smoothed_y[-1]:.4g}; lr {learning_rates[-1]:.4g}'\n",
    "\n",
    "            # Display selected images\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = model(valid_x)\n",
    "            model.train()\n",
    "            for img_idx, img in enumerate(pred):\n",
    "                buffer = io.BytesIO()\n",
    "                ToPILImage()(img).resize((384,384)).save(buffer, format=\"PNG\")\n",
    "                buffer.seek(0)\n",
    "                if len(img_displays) <= img_idx:\n",
    "                    img_displays.append(display(IPython.display.Image(buffer.read()), display_id=True))\n",
    "                else:\n",
    "                    IPython.display.update_display(IPython.display.Image(buffer.read()), display_id=img_displays[img_idx].display_id)\n",
    "\n",
    "        i_step+=1\n",
    "        \n",
    "    # validation\n",
    "    model.eval()\n",
    "    valid_loss.append(0)\n",
    "    rand_sample = torch.randint(low=0, high=dataset_valid.num_rows,size=(config.valid_samples_per_epoch,))\n",
    "    data_loader_valid = torch.utils.data.DataLoader(\n",
    "        dataset_valid.select(rand_sample),\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "        collate_fn=valid_collate_fn\n",
    "    )\n",
    "    pb_valid = progress_bar(data_loader_valid, parent=mb)\n",
    "    valid_loss_per_sample = []\n",
    "    for i_batch, (x,y) in enumerate(pb_valid):\n",
    "        x = x.to(torch.float)\n",
    "        x = x / 255\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "            for i_valid, yi in enumerate(y):\n",
    "                pred_i = Resize(size=(yi.height, yi.width))(pred[i_valid])\n",
    "                yi = PILToTensor()(yi)\n",
    "                yi = yi.to(torch.float)\n",
    "                yi = yi / 255\n",
    "                yi = yi.to(device)\n",
    "                loss = nn.functional.mse_loss(pred_i, yi)\n",
    "                valid_loss_per_sample.append(loss.item())\n",
    "    valid_loss[-1] = np.median(valid_loss_per_sample)\n",
    "    mb.main_bar.comment = f'valid_loss {valid_loss[-1]:.4g}'\n",
    "    mb.child.comment = f'loss {smoothed_y[-1]:.4g}; lr {learning_rates[-1]:.4g}'\n",
    "\n",
    "    torch.save({\n",
    "        'i_epoch': i_epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'learning_rates': learning_rates,\n",
    "        'smoothed_y': smoothed_y,\n",
    "        'valid_loss': valid_loss,\n",
    "        'config': config,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }, f\"log_{device}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
