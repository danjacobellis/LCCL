{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e03ec-a0d6-460d-87e6-43556547d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/danjacobellis/walloc/resolve/main/RGB_Li_48c_J3_nf8_v1.0.2.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae16cc7-9519-4223-8ba2-3f65f134600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import warnings\n",
    "import IPython.display\n",
    "import io\n",
    "import random\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from datasets import load_dataset\n",
    "from torchvision.transforms.v2 import (\n",
    "    RandomResizedCrop, Resize, CenterCrop,\n",
    "    PILToTensor, ToPILImage, MixUp,\n",
    "    Compose, RandomHorizontalFlip )\n",
    "from vit_pytorch.simple_vit_with_register_tokens import SimpleViT\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from walloc import walloc\n",
    "class Config: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c86408-6457-4ece-9b44-c1635850a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:2\"\n",
    "\n",
    "checkpoint = torch.load(\"RGB_Li_48c_J3_nf8_v1.0.2.pth\",map_location=\"cpu\",weights_only=False)\n",
    "codec_config = checkpoint['config']\n",
    "codec = walloc.Codec2D(\n",
    "    channels = codec_config.channels,\n",
    "    J = codec_config.J,\n",
    "    Ne = codec_config.Ne,\n",
    "    Nd = codec_config.Nd,\n",
    "    latent_dim = codec_config.latent_dim,\n",
    "    latent_bits = codec_config.latent_bits,\n",
    "    lightweight_encode = codec_config.lightweight_encode\n",
    ")\n",
    "codec.load_state_dict(checkpoint['model_state_dict'])\n",
    "codec = codec.to(device)\n",
    "codec.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b2aae6-9b16-4dc2-86ef-79d5a6093fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "# Training and optimizer config\n",
    "config.batch_size = 128\n",
    "config.grad_accum_steps = 1\n",
    "config.min_lr = 1e-6\n",
    "config.max_lr = 4e-4\n",
    "config.warmup_steps = 50000\n",
    "config.plot_update = 128\n",
    "config.patience = 64\n",
    "config.weight_decay = 0.\n",
    "config.epochs = 100\n",
    "config.num_workers = 12\n",
    "config.valid_image_size = 288\n",
    "config.mixup_probability = 0.2\n",
    "config.mixup_alpha = 0.8\n",
    "\n",
    "# Classification model config\n",
    "config.image_size = 256\n",
    "config.channels = 3\n",
    "config.patch_size = 16\n",
    "config.num_classes = 1000\n",
    "config.embed_dim = 192\n",
    "config.depth = 12\n",
    "config.heads = 3\n",
    "config.mlp_dim = 768\n",
    "config.dim_head = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7c35459-ecdb-4933-a741-eb188f573596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.561128"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleViT(\n",
    "    image_size = config.image_size//(2**codec_config.J),\n",
    "    channels = codec_config.latent_dim,\n",
    "    patch_size = config.patch_size//(2**codec_config.J),\n",
    "    num_classes = config.num_classes,\n",
    "    dim = config.embed_dim,\n",
    "    depth = config.depth,\n",
    "    heads = config.heads,\n",
    "    mlp_dim = config.mlp_dim,\n",
    "    dim_head = config.dim_head\n",
    ").to(device)\n",
    "sum(p.numel() for p in model.parameters())/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b87f73a-8824-434a-824f-ba90057bc338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cfa6ff599141a5a87d90d4ef8a4947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_train = load_dataset('imagenet-1k',split='train',trust_remote_code=True)\n",
    "dataset_valid = load_dataset('imagenet-1k',split='validation',trust_remote_code=True)\n",
    "dataset_valid2 = load_dataset('danjacobellis/imagenet-a', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6767f4d3-3df3-4a67-a9a5-bf71998f8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_transform = MixUp(num_classes=config.num_classes, alpha=config.mixup_alpha)\n",
    "\n",
    "L = config.image_size\n",
    "C = config.channels\n",
    "\n",
    "train_transform = Compose([\n",
    "    RandomResizedCrop(\n",
    "        size=(L,L),\n",
    "        interpolation=Image.Resampling.LANCZOS\n",
    "    ),\n",
    "    RandomHorizontalFlip(0.5),\n",
    "    PILToTensor()\n",
    "])\n",
    "\n",
    "valid_transform = Compose([\n",
    "    Resize(\n",
    "        size=config.valid_image_size,\n",
    "        interpolation=Image.Resampling.LANCZOS\n",
    "    ),\n",
    "    CenterCrop(size=L),\n",
    "    PILToTensor(),\n",
    "])\n",
    "\n",
    "def train_collate_fn(batch):\n",
    "    B = len(batch)    \n",
    "    x = torch.zeros( (B, C, L, L), dtype=torch.uint8)\n",
    "    y = torch.zeros(B, dtype=torch.int)\n",
    "    for i_sample, sample in enumerate(batch):\n",
    "        img = sample['image'].convert(\"RGB\")\n",
    "        x[i_sample,:,:,:] = train_transform(img)\n",
    "        y[i_sample] = sample['label']\n",
    "    return x, y\n",
    "\n",
    "def valid_collate_fn(batch):\n",
    "    B = len(batch)    \n",
    "    x = torch.zeros( (B, C, L, L), dtype=torch.uint8)\n",
    "    y = torch.zeros(B, dtype=torch.int)\n",
    "    for i_sample, sample in enumerate(batch):\n",
    "        img = sample['image'].convert(\"RGB\")\n",
    "        x[i_sample,:,:,:] = valid_transform(img)\n",
    "        y[i_sample] = sample['label']\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cedac284-9bce-46a1-948a-36693e204d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=config.min_lr,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "def minus_cosine_warmup(i_step):\n",
    "    scale = 0.5 * (np.log10(config.max_lr) - np.log10(config.min_lr))\n",
    "    angle =  np.pi * i_step / (config.warmup_steps//config.plot_update)\n",
    "    log_lr = np.log10(config.min_lr) + scale * (1 - np.cos(angle))\n",
    "    lr = 10 ** log_lr\n",
    "    return lr/config.min_lr\n",
    "    \n",
    "warmup = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda = lambda i_step: minus_cosine_warmup(i_step)\n",
    ")\n",
    "\n",
    "reduce_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    factor=0.98,\n",
    "    patience=config.patience,\n",
    "    threshold=1e-3,\n",
    "    min_lr=config.min_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f7abb-cb89-49fe-9438-8eb76fedda14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/100 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='115' class='' max='10009' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.15% [115/10009 00:19&lt;27:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\"Corrupt EXIF data.  Expecting to read .* bytes but only got .*\", category=UserWarning, module=\"PIL.TiffImagePlugin\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Metadata Warning, tag .* had too many entries: .* expected .*\", category=UserWarning, module=\"PIL.TiffImagePlugin\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Truncated File Read\", category=UserWarning, module=\"PIL.TiffImagePlugin\")\n",
    "\n",
    "mb = master_bar(range(config.epochs))\n",
    "mb.names = ['per batch','smoothed']\n",
    "train_loss = []\n",
    "valid_acc = []\n",
    "valid_acc_a = []\n",
    "learning_rates = [optimizer.param_groups[0]['lr']]\n",
    "i_step = 0\n",
    "for i_epoch in mb:\n",
    "    # training\n",
    "    model.train()\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers,\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "        collate_fn=train_collate_fn\n",
    "    )\n",
    "    pb = progress_bar(dataloader_train, parent=mb)\n",
    "    for i_batch, (x,y) in enumerate(pb):\n",
    "        \n",
    "        x = x.to(torch.float)\n",
    "        x = x/255\n",
    "        x = x.to(device)\n",
    "        y = y.to(torch.long).to(device)\n",
    "\n",
    "        # mixup\n",
    "        if random.random() < config.mixup_probability:\n",
    "            x, y = mixup_transform(x, y)\n",
    "\n",
    "        # Compress on the fly\n",
    "        x = x-0.5        \n",
    "        with torch.no_grad():\n",
    "            x = codec.wavelet_analysis(x,codec.J)\n",
    "            x = codec.encoder[:2](x)\n",
    "            \n",
    "        # forward\n",
    "        logits = model(x)\n",
    "        loss = nn.CrossEntropyLoss()(input=logits, target=y)\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        if (i_batch + 1) % config.grad_accum_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # plotting\n",
    "        if (i_step+1) % config.plot_update == 0:\n",
    "            plot_n = len(train_loss) // config.plot_update\n",
    "            smoothed_x = (0.5+torch.arange(plot_n)) * config.plot_update\n",
    "            smoothed_y = torch.tensor(train_loss).reshape(plot_n, -1).mean(dim=1)\n",
    "            train_x = range(len(train_loss))\n",
    "            train_y = train_loss\n",
    "            mb.update_graph([[train_x, np.log10(train_y)],[smoothed_x, np.log10(smoothed_y)]])\n",
    "\n",
    "            # lr scheduler\n",
    "            if i_step < config.warmup_steps:\n",
    "                warmup.step()\n",
    "            else:\n",
    "                reduce_plateau.step(smoothed_y[-1])\n",
    "            learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "        i_step+=1\n",
    "        \n",
    "    # validation\n",
    "    model.eval()\n",
    "    valid_acc.append(0)\n",
    "    valid_acc_a.append(0)\n",
    "    data_loader_valid = torch.utils.data.DataLoader(\n",
    "        dataset_valid,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        collate_fn=valid_collate_fn\n",
    "    )\n",
    "    \n",
    "    data_loader_valid2 = torch.utils.data.DataLoader(\n",
    "        dataset_valid2,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        collate_fn=valid_collate_fn\n",
    "    )\n",
    "\n",
    "    # standard validation set\n",
    "    pb_valid = progress_bar(data_loader_valid, parent=mb)\n",
    "    for i_batch, (x, y) in enumerate(pb_valid):\n",
    "        y = y.to(torch.long).to(device)\n",
    "        x = x.to(torch.float)\n",
    "        x = x / 255\n",
    "        x = x - 0.5\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            x = codec.wavelet_analysis(x, codec.J)\n",
    "            x = codec.encoder[:2](x)\n",
    "            logits = model(x)\n",
    "        valid_acc[i_epoch] += torch.sum(logits.argmax(dim=1) == y).item()\n",
    "    valid_acc[i_epoch] /= len(dataset_valid)\n",
    "    \n",
    "    # adversarial validation set\n",
    "    pb_valid2 = progress_bar(data_loader_valid2, parent=mb)\n",
    "    for i_batch, (x, y) in enumerate(pb_valid2):\n",
    "        y = y.to(torch.long).to(device)\n",
    "        x = x.to(torch.float)\n",
    "        x = x / 255\n",
    "        x = x - 0.5\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            x = codec.wavelet_analysis(x, codec.J)\n",
    "            x = codec.encoder[:2](x)\n",
    "            logits = model(x)\n",
    "        valid_acc_a[i_epoch] += torch.sum(logits.argmax(dim=1) == y).item()\n",
    "    valid_acc_a[i_epoch] /= len(dataset_valid2)\n",
    "    \n",
    "    mb.main_bar.comment = f'valid_acc {valid_acc[i_epoch]:.4g}, valid_acc_a {valid_acc_a[i_epoch]:.4g}'\n",
    "    \n",
    "    torch.save({\n",
    "        'i_epoch': i_epoch,\n",
    "        'learning_rates': learning_rates,\n",
    "        'smoothed_y': smoothed_y,\n",
    "        'valid_acc': valid_acc,\n",
    "        'valid_acc_a': valid_acc_a,\n",
    "        'config': config\n",
    "    }, f\"log_{device}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb020a6-a19e-4296-87bd-475fafd28cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, update_display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "display(HTML(mb.main_bar.progress))\n",
    "display(HTML(mb.child.progress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d0bc2a-a471-457a-96e0-5a9e8001c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_train_loss = 10 * (7 - smoothed_y)\n",
    "valid_acc_pct = 100*torch.tensor(valid_acc)\n",
    "fig, ax1 = plt.subplots()\n",
    "line1, = ax1.plot(fn_train_loss, label='Scaled (7 - Smoothed Y)', color='black')\n",
    "ax1.set_xlabel('Steps')\n",
    "ax1.tick_params(axis='x', colors='black')\n",
    "ax1.tick_params(axis='y', colors='black')\n",
    "ax2 = ax1.twiny()\n",
    "line2, = ax2.plot(valid_acc_pct, label='Validation Accuracy', color='blue')\n",
    "ax2.set_xlabel('Epochs', color='blue')  # Label for the top x-axis\n",
    "ax2.tick_params(axis='x', colors='blue')\n",
    "ax2.tick_params(axis='y', colors='blue')\n",
    "ax1.legend(handles=[line1, line2], labels=['âˆ(7 - training loss) [Nats]', 'Validation Accuracy [%]'], loc='lower right')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff65524-767b-4466-8782-08b6977ebdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d961694-148d-40e9-ae57-9a2663de9003",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'i_epoch': i_epoch,\n",
    "    'learning_rates': learning_rates,\n",
    "    'smoothed_y': smoothed_y,\n",
    "    'valid_acc': valid_acc,\n",
    "    'valid_acc_a': valid_acc_a,\n",
    "    'config': config\n",
    "}, f\"../../hf/LCCL/classification_walloc_mixup.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
