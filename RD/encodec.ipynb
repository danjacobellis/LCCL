{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b6a3b0-bc00-4ec5-b0b2-3dc34104bfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=name --format=csv,noheader | head -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d7db02c-516f-4ddd-ba4a-31b7dcc18526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import cdpam\n",
    "from IPython.display import Audio\n",
    "from transformers import EncodecModel, AutoProcessor\n",
    "from torchvision.transforms import ToPILImage, PILToTensor\n",
    "from datasets import load_dataset, Image \n",
    "from walloc import walloc\n",
    "from spauq.core.metrics import spauq_eval\n",
    "class Config: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d8daf3-5777-4f61-b570-cf77fb459585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "codec = EncodecModel.from_pretrained(\"facebook/encodec_48khz\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_48khz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfc6e03e-ffcc-443f-be69-f6f75ba737e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/cdpam/cdpam.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(modfolder,map_location=\"cpu\")['state']\n"
     ]
    }
   ],
   "source": [
    "cdpam_loss = cdpam.CDPAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eea68b9d-1e5a-432f-af4a-8ad4447fcf3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25afbd2e9944db5b5828a80793bfd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2938253dd645f79c8bf7af44a567d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b255619bae0545508b199b8b45bf52e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f53be39c3c4a0a9b0767208210fb7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdc84fa6431408c96c86f6f251849fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MUSDB = load_dataset(\"danjacobellis/musdb_segments\", split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8024e4dc-ba66-4c7c-a543-642a0e45c33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(audio, p=2**16):\n",
    "    B,C,L = audio.shape\n",
    "    padding_size = (p - (L % p)) % p\n",
    "    if padding_size > 0:\n",
    "        audio = torch.nn.functional.pad(audio, (0, padding_size), mode='constant', value=0)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff28625f-1a1a-465c-ac61-404d14ea35cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodec_compress(sample):\n",
    "    with torch.no_grad():\n",
    "        x, fs = torchaudio.load(sample['audio_mix']['bytes'],normalize=False)\n",
    "        x = x.to(torch.float)\n",
    "        x = x - x.mean()\n",
    "        max_abs = x.abs().max()\n",
    "        x = x / (max_abs + 1e-8)\n",
    "        x = x/2\n",
    "        L = x.shape[-1]\n",
    "    \n",
    "        t0 = time.time()\n",
    "        x_pre = torchaudio.transforms.Resample(fs,processor.sampling_rate)(2*x)\n",
    "        x_pre = processor(raw_audio=x_pre, sampling_rate=processor.sampling_rate, return_tensors=\"pt\")\n",
    "        Y = codec.encode(\n",
    "            input_values = x_pre['input_values'].to(device),\n",
    "            padding_mask = x_pre['padding_mask'].to(device),\n",
    "            bandwidth=12.0,\n",
    "        )\n",
    "        Y['audio_codes'] = Y['audio_codes'].to(\"cpu\")\n",
    "        encode_time = time.time() - t0\n",
    "    \n",
    "        t0 = time.time()\n",
    "        x_hat = codec.decode(\n",
    "            audio_codes=Y['audio_codes'].to(device),\n",
    "            audio_scales=Y['audio_scales'],\n",
    "            padding_mask=x_pre['padding_mask']\n",
    "        )['audio_values']\n",
    "        x_hat = torchaudio.transforms.Resample(processor.sampling_rate,fs).to(device)(x_hat)\n",
    "        x_hat = x_hat / 2\n",
    "        x_hat = x_hat[0,:,:L].clamp(-0.5, 0.5)\n",
    "        decode_time = time.time() - t0\n",
    "    \n",
    "        bps = 10*Y['audio_codes'].numel()/(x.numel())\n",
    "        PSNR = -10*np.log10(torch.nn.functional.mse_loss(x,x_hat.to(\"cpu\")))\n",
    "        SDR = spauq_eval(x,x_hat.to(\"cpu\"),fs=fs)\n",
    "        SSDR = SDR['SSR']\n",
    "        SRDR = SDR['SRR']\n",
    "        cdpam = cdpam_loss.forward(x.to(device),x_hat).mean().item()\n",
    "\n",
    "    return {\n",
    "        'compressed': Y,\n",
    "        'encode_time': encode_time,\n",
    "        'decode_time': decode_time,\n",
    "        'bps': bps,\n",
    "        'L': L,\n",
    "        'PSNR': PSNR,\n",
    "        'SSDR': SSDR,\n",
    "        'SRDR': SRDR,\n",
    "        'CDPAM': cdpam\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff8eb8f0-43f8-46bc-8418-62c9f207caa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f9f4ea028247a381e74d22a8b19131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/spauq/core/preprocessing.py:325: UserWarning: No forgive_mode specified, defaulting to `none`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "codec = codec.to(device)\n",
    "gpu = MUSDB.map(\n",
    "    encodec_compress,\n",
    "    writer_batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8399eb6d-c35b-4cbe-88f4-1d3f1353bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodec_compress_cpu(sample):\n",
    "    with torch.no_grad():\n",
    "        x, fs = torchaudio.load(sample['audio_mix']['bytes'],normalize=False)\n",
    "        x = x.to(torch.float)\n",
    "        x = x - x.mean()\n",
    "        max_abs = x.abs().max()\n",
    "        x = x / (max_abs + 1e-8)\n",
    "        x = x/2\n",
    "        L = x.shape[-1]\n",
    "    \n",
    "        t0 = time.time()\n",
    "        x_pre = torchaudio.transforms.Resample(fs,processor.sampling_rate)(2*x)\n",
    "        x_pre = processor(raw_audio=x_pre, sampling_rate=processor.sampling_rate, return_tensors=\"pt\")\n",
    "        Y = codec.encode(\n",
    "            input_values = x_pre['input_values'].to(device),\n",
    "            padding_mask = x_pre['padding_mask'].to(device),\n",
    "            bandwidth=12.0,\n",
    "        )\n",
    "        Y['audio_codes'] = Y['audio_codes'].to(\"cpu\")\n",
    "        encode_time = time.time() - t0\n",
    "    \n",
    "        t0 = time.time()\n",
    "        x_hat = codec.decode(\n",
    "            audio_codes=Y['audio_codes'].to(device),\n",
    "            audio_scales=Y['audio_scales'],\n",
    "            padding_mask=x_pre['padding_mask']\n",
    "        )['audio_values']\n",
    "        x_hat = torchaudio.transforms.Resample(processor.sampling_rate,fs).to(device)(x_hat)\n",
    "        x_hat = x_hat / 2\n",
    "        x_hat = x_hat[0,:,:L].clamp(-0.5, 0.5)\n",
    "        decode_time = time.time() - t0\n",
    "\n",
    "    return {\n",
    "        'cpu_encode_time': encode_time,\n",
    "        'cpu_decode_time': decode_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e8db5a5-7c2b-4350-8192-5e4bd510616a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31725df8fe63421cb3eaaa87a565add7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "codec.eval();\n",
    "codec = codec.to(device)\n",
    "cpu = MUSDB.map(encodec_compress_cpu, writer_batch_size=16)\n",
    "combined = gpu.add_column('cpu_encode_time',cpu['cpu_encode_time'])\n",
    "combined = combined.add_column('cpu_decode_time',cpu['cpu_decode_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f58c9ef6-d925-48a0-81e3-4d4f029f892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'encode_time',\n",
    "    'decode_time',\n",
    "    'bps',\n",
    "    'PSNR',\n",
    "    'SSDR',\n",
    "    'SRDR',\n",
    "    'CDPAM',\n",
    "    'cpu_encode_time',\n",
    "    'cpu_decode_time',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a79683ca-4a61-4940-9c65-e680df7e1ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode_time: 0.3939989122725625\n",
      "decode_time: 0.33626718284519574\n",
      "bps: 0.14019012451171875\n",
      "PSNR: 31.95164082796519\n",
      "SSDR: 22.69493529722898\n",
      "SRDR: 6.691856596921452\n",
      "CDPAM: 2.065623472935146e-05\n",
      "cpu_encode_time: 1.5419972561697923\n",
      "cpu_decode_time: 1.4067453200580509\n"
     ]
    }
   ],
   "source": [
    "for metric in metrics:\n",
    "    μ = np.mean(combined[metric])\n",
    "    print(f\"{metric}: {μ}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe68441-e2b6-4e1c-9f21-a039b5a35e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.push_to_hub(\"danjacobellis/MUSDB_encodec_12kbps\",split='validation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
