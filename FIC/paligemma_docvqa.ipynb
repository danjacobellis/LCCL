{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01bfb329-be53-457a-a199-5c0492274190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from datasets import Image as HFImage\n",
    "from anls_star import anls_score\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "from torchvision.transforms import ToPILImage, PILToTensor\n",
    "from walloc import walloc\n",
    "class Config: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f235d0-4e25-4613-8228-39d14af10f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e93d6098e1845dd9b9f7ee723f39ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77910184e9ce4af080054516070746aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/transformers/models/paligemma/configuration_paligemma.py:137: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.44, Please use `text_config.vocab_size` instead.\n",
      "  warnings.warn(\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad0a8a40d5142c8a750747c7bfc3cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"danjacobellis/docvqa\",split='test')\n",
    "model_id = \"google/paligemma-3b-ft-docvqa-448\"\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\",\n",
    "    revision=\"bfloat16\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6efb339-94bb-4e26-b8a9-b753cc270ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(item,res):\n",
    "    prompt = item['question']\n",
    "    image = item['image'].resize((res, res), resample=Image.Resampling.LANCZOS).convert(\"RGB\")\n",
    "    model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n",
    "    input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "    with torch.inference_mode():\n",
    "        generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
    "        generation = generation[0][input_len:]\n",
    "        pred = processor.decode(generation, skip_special_tokens=True)\n",
    "    score, _ = anls_score(item['answer'], pred, return_gt=True)\n",
    "    return score, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf90fe72-b4cd-4c64-99f0-9296c2b0b06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='5188' class='' max='5188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5188/5188 10:56&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/anls_star/anls_star.py:513: UserWarning: Treating ground truth as a list of options. This is a compatibility mode for ST-VQA-like datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "preds = []\n",
    "qid = []\n",
    "res = 448\n",
    "for item in progress_bar(ds):\n",
    "    score, pred = compute_score(item,res)\n",
    "    scores.append(score)\n",
    "    preds.append(pred)\n",
    "    qid.append(item['questionId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be546396-d3d2-4e5d-a25b-f1cc91d1b828",
   "metadata": {},
   "source": [
    "9953 MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e98d3fd0-7de6-43c7-bbfb-f3a35314b2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.908536585365853"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5188 / (10*60+56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc91b0-af89-497a-a65b-dca473bb422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.add_column('preds',preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34982e6b-e542-4e0e-b348-fee367ff6bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.push_to_hub(\"danjacobellis/docvqa_paligemma224\",split='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
