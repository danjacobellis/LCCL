{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b575600f-e76e-4098-8b05-86bbee490c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display, Audio, Image, update_display, HTML\n",
    "from torchvision.transforms import ToPILImage\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from audio_mvrt import MaxViT1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f258bd-068c-455b-b854-631de4a5e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "class Config: pass\n",
    "config = Config()\n",
    "config.batch_size = 128\n",
    "config.num_workers = 36\n",
    "config.grad_accum_steps = 1\n",
    "config.plot_update = 64\n",
    "config.patience = 64\n",
    "config.min_lr = 1e-7\n",
    "config.max_lr = 3e-5\n",
    "config.warmup_steps = 5000\n",
    "config.weight_decay = 0.\n",
    "config.epochs = 30\n",
    "config.epoch_len = 10000\n",
    "\n",
    "config.length_samples = 2**18\n",
    "config.patch_size = config.length_samples//256\n",
    "config.channels = 2\n",
    "config.num_classes = config.channels*config.patch_size\n",
    "config.embed_dim = 192\n",
    "config.depth = (4,)\n",
    "config.downsample = False\n",
    "config.heads = 3\n",
    "config.mlp_dim = 768\n",
    "config.dim_head = 64\n",
    "config.dim_conv_stem = 192\n",
    "config.window_size = 16\n",
    "config.mbconv_expansion_rate = 4\n",
    "config.mbconv_shrinkage_rate = 0.25\n",
    "config.dropout = 0.0\n",
    "config.num_register_tokens = 4\n",
    "config.dense_prediction = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d68e8a-faf6-42a9-8212-7b0c6550886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaxViT1D(\n",
    "    channels = config.channels,\n",
    "    patch_size = config.patch_size,\n",
    "    num_classes = config.num_classes,\n",
    "    dim = config.embed_dim,\n",
    "    depth = config.depth,\n",
    "    downsample = config.downsample,\n",
    "    # heads = config.heads, # calculated as dim//dim_head  \n",
    "    # mlp_dim = config.mlp_dim, # calculated as 4*dim\n",
    "    dim_head = config.dim_head,\n",
    "    dim_conv_stem = config.dim_conv_stem,\n",
    "    window_size = config.window_size,\n",
    "    mbconv_expansion_rate = config.mbconv_expansion_rate,\n",
    "    mbconv_shrinkage_rate = config.mbconv_shrinkage_rate,\n",
    "    dropout = config.dropout,\n",
    "    num_register_tokens = config.num_register_tokens,\n",
    "    dense_prediction=True\n",
    ").to(device)\n",
    "\n",
    "sum(p.numel() for p in model.parameters())/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca760e-9293-4509-b159-e3fc78268bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"danjacobellis/musdb18hq_vss\",split='train')\n",
    "valid_dataset = load_dataset(\"danjacobellis/musdb18hq_vss\",split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645876ab-19e6-4653-a013-f2eb71b24a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = config.length_samples\n",
    "C = config.channels\n",
    "crop = torchvision.transforms.RandomCrop((2,L))\n",
    "def collate_fn(batch):\n",
    "    B = len(batch)\n",
    "    x = torch.zeros( (B, C, 2, L), dtype=torch.float32)\n",
    "    i_sample = 0\n",
    "    for i_sample, sample in enumerate(batch):\n",
    "        audio_mix, fs = torchaudio.load(sample['audio_mix']['bytes'])\n",
    "        audio_vocal, fs = torchaudio.load(sample['audio_vocal']['bytes'])\n",
    "        audio_mix = audio_mix.unsqueeze(1)\n",
    "        audio_vocal = audio_vocal.unsqueeze(1)\n",
    "        audio = torch.cat([audio_mix,audio_vocal],dim=1)\n",
    "        x[i_sample,:,:,:] = crop(audio)\n",
    "    return x[:,:,0,:], x[:,:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a6efe-42fa-47de-a1d0-72343cc9d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batch = valid_dataset[89:90]\n",
    "valid_batch = [dict(zip(valid_batch.keys(), values)) for values in zip(*valid_batch.values())]\n",
    "x_valid, v_valid = collate_fn(valid_batch)\n",
    "x_valid = x_valid.to(device)\n",
    "v_valid = v_valid.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8a7ea-7f28-410c-bf44-a7cc0b10a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spectrogram(X):\n",
    "    X = spectrogram(X).log()\n",
    "    X = X - X.mean()\n",
    "    X = X/X.std()\n",
    "    X = X/3\n",
    "    X = X.clamp(-0.5,0.5)\n",
    "    X = X + 0.5\n",
    "    return ToPILImage()(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d954252-a8bc-4043-85fa-2298010cec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=44100,\n",
    "    n_fft=4096,\n",
    ").to(device)\n",
    "SG = make_spectrogram(x_valid[0,0].to(device))\n",
    "SG.resize((256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fde17e-5a21-4ae4-b9df-c98f4d89d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(x_valid.to(\"cpu\").numpy()[0],rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf36f5b-ffad-4e95-b640-a1aad7a89d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    weight_decay=0.0,\n",
    "    lr = config.min_lr\n",
    ")\n",
    "\n",
    "def minus_cosine_warmup(i_step):\n",
    "    scale = 0.5 * (np.log10(config.max_lr) - np.log10(config.min_lr))\n",
    "    angle =  np.pi * i_step / (config.warmup_steps//config.plot_update)\n",
    "    log_lr = np.log10(config.min_lr) + scale * (1 - np.cos(angle))\n",
    "    lr = 10 ** log_lr\n",
    "    return lr/config.min_lr\n",
    "    \n",
    "warmup = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda = lambda i_step: minus_cosine_warmup(i_step)\n",
    ")\n",
    "\n",
    "reduce_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    factor=0.98,\n",
    "    patience=config.patience,\n",
    "    threshold=1e-5,\n",
    "    min_lr=config.min_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc90cb48-6ce0-4744-ad54-3cec88cfc78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "learning_rates = [optimizer.param_groups[0]['lr']]\n",
    "img_displays = []\n",
    "text_display = None\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "mb = master_bar(range(config.epochs))\n",
    "mb.names = ['Loss', 'Smoothed']\n",
    "i_step = 0\n",
    "for i_epoch in mb:\n",
    "    sampler = torch.utils.data.RandomSampler(\n",
    "        train_dataset,\n",
    "        replacement=True,\n",
    "        num_samples=config.epoch_len*config.batch_size\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        num_workers=config.num_workers,\n",
    "        sampler=sampler,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "   \n",
    "    for i, (x,v) in enumerate(progress_bar(dataloader, parent=mb)):\n",
    "        x = x.to(device)\n",
    "        v = v.to(device)\n",
    "\n",
    "        pred = model(x)\n",
    "        mse_loss = torch.nn.functional.mse_loss(pred, x-v)\n",
    "        losses.append(np.log10(mse_loss.item()))\n",
    "        mse_loss.backward()\n",
    "        if (i + 1) % config.grad_accum_steps == 0: \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # plotting and lr scheduler\n",
    "        if len(losses) % config.plot_update == 0:\n",
    "            plot_n = len(losses) // config.plot_update\n",
    "            smoothed_x = (0.5+torch.arange(plot_n)) * config.plot_update\n",
    "            smoothed_y = torch.tensor(losses).reshape(plot_n, -1).mean(dim=1)\n",
    "            dist_x = range(len(losses))\n",
    "            dist_y = losses\n",
    "            mb.update_graph([[dist_x, dist_y],[smoothed_x, smoothed_y]])\n",
    "            mb.child.comment = f'loss {smoothed_y[-1]:.4g}; lr {learning_rates[-1]:.4g}'\n",
    "\n",
    "            # lr scheduler\n",
    "            if i_step < config.warmup_steps:\n",
    "                warmup.step()\n",
    "            else:\n",
    "                reduce_plateau.step(smoothed_y[-1])\n",
    "            learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                y_valid = model(x_valid)[0]\n",
    "                torchaudio.save(f\"valid_{device}.wav\", src=y_valid.to(\"cpu\"), sample_rate=44100)\n",
    "                buffer = io.BytesIO()\n",
    "                make_spectrogram(y_valid).resize((1026,256)).save(buffer, format=\"PNG\")\n",
    "                model.train()\n",
    "            buffer.seek(0)\n",
    "            if len(img_displays) == 0:\n",
    "                img_displays.append(display(Image(buffer.read()), display_id=True))\n",
    "            else:\n",
    "                update_display(Image(buffer.read()), display_id=img_displays[0].display_id)\n",
    "        i_step+=1\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'i_epoch': i_epoch,\n",
    "            'learning_rates': learning_rates,\n",
    "            'train_loss': losses,\n",
    "            'config': config\n",
    "        }, f\"log_{device}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a217d82-bb67-46a0-b880-f384e169173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, update_display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "display(HTML(mb.main_bar.progress))\n",
    "display(HTML(mb.child.progress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3be0d5-9f75-4847-a37a-ea6e3f6ff09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    y_valid = model(x_valid)[0]\n",
    "    torchaudio.save(f\"valid_{device}.wav\", src=y_valid.to(\"cpu\"), sample_rate=44100)\n",
    "    buffer = io.BytesIO()\n",
    "    make_spectrogram(y_valid).resize((1026,256)).save(buffer, format=\"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9deded2-13c8-47aa-b61e-be3af4ccdbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.seek(0)\n",
    "display(Image(buffer.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37f9d1a-844d-4439-a785-421f8513e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_audio = y_valid.to(\"cpu\").numpy()[0]\n",
    "Audio(valid_audio,rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995c4ee-ab61-4d4d-baee-336af815a2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
