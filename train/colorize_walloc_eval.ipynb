{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b9725-15c1-4c74-bffe-cfc4286b2ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://hf.co/danjacobellis/walloc/resolve/main/RGB_Li_48c_J3_nf8_v1.0.2.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d1165-5526-45be-9844-d453f726aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://hf.co/danjacobellis/walloc/resolve/main/RGB_Li_12c_J3_nf8_v1.0.2.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c291d86-d3e5-49df-bb59-c811b91e43c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/danjacobellis/LCCL/resolve/main/colorize_walloc_4x_256_p16.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c9037a-0efe-4a29-b88d-7b1d580e568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/danjacobellis/LCCL/resolve/main/colorize_walloc_16x_512_p32.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b73022d-e3f7-47b8-8fb6-cb7615d1445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import warnings\n",
    "import IPython.display\n",
    "import io\n",
    "import time\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from datasets import load_dataset\n",
    "from datasets import Image as HFImage\n",
    "from torchvision.transforms import (\n",
    "    RandomResizedCrop, Resize, Grayscale,\n",
    "    PILToTensor, ToPILImage, \n",
    "    Compose, RandomHorizontalFlip )\n",
    "from max_vit_with_register_tokens import MaxViT\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from piq import LPIPS, DISTS, psnr, multi_scale_ssim\n",
    "from walloc import walloc\n",
    "class Config: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cad039e-4192-4099-957c-d822b0eb2ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "checkpoint = torch.load(\"RGB_Li_12c_J3_nf8_v1.0.2.pth\",map_location=\"cpu\",weights_only=False)\n",
    "codec_config = checkpoint['config']\n",
    "codec = walloc.Codec2D(\n",
    "    channels = codec_config.channels,\n",
    "    J = codec_config.J,\n",
    "    Ne = codec_config.Ne,\n",
    "    Nd = codec_config.Nd,\n",
    "    latent_dim = codec_config.latent_dim,\n",
    "    latent_bits = codec_config.latent_bits,\n",
    "    lightweight_encode = codec_config.lightweight_encode\n",
    ")\n",
    "codec.load_state_dict(checkpoint['model_state_dict'])\n",
    "codec = codec.to(device)\n",
    "codec.eval();\n",
    "\n",
    "checkpoint = torch.load(\"colorize_walloc_16x_512_p32.pth\",map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "model = MaxViT(\n",
    "    channels = codec_config.latent_dim,\n",
    "    patch_size = config.patch_size//(2**codec_config.J),\n",
    "    num_classes = config.num_classes,\n",
    "    dim = config.embed_dim,\n",
    "    depth = config.depth,\n",
    "    downsample = config.downsample,\n",
    "    # heads = config.heads, # calculated as dim//dim_head  \n",
    "    # mlp_dim = config.mlp_dim, # calculated as 4*dim\n",
    "    dim_head = config.dim_head,\n",
    "    dim_conv_stem = config.dim_conv_stem,\n",
    "    window_size = config.window_size,\n",
    "    mbconv_expansion_rate = config.mbconv_expansion_rate,\n",
    "    mbconv_shrinkage_rate = config.mbconv_shrinkage_rate,\n",
    "    dropout = config.dropout,\n",
    "    num_register_tokens = config.num_register_tokens,\n",
    "    dense_prediction=True\n",
    ").to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc9989d-321b-4a0b-9c39-f1027a18cfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "lpips_loss = LPIPS().to(\"cuda\")\n",
    "dists_loss = DISTS().to(\"cuda\")\n",
    "gpu_mem_baseline = torch.cuda.memory_reserved(0)/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0063a9f4-9f41-4c5c-9024-a751e043b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_valid = load_dataset('danjacobellis/LSDIR_val',split='validation',trust_remote_code=True)\n",
    "valid_transform = Compose([\n",
    "    Resize(\n",
    "        size=(config.image_size,config.image_size),\n",
    "        interpolation=Image.Resampling.LANCZOS\n",
    "    ),\n",
    "    PILToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f54bdbbd-d267-4752-b196-0921d32a8752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize_gpu(sample):\n",
    "    with torch.no_grad():\n",
    "        img = sample['image'].convert(\"RGB\")\n",
    "        y = PILToTensor()(img)\n",
    "        x = Grayscale(num_output_channels=3)(valid_transform(img))\n",
    "        x = x.to(torch.float)\n",
    "        y = y.to(torch.float)\n",
    "        x = (x / 255) - 0.5\n",
    "        y = y / 255\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        x = x.unsqueeze(0)\n",
    "        y = y.unsqueeze(0)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        x = codec.wavelet_analysis(x,codec.J)\n",
    "        x = codec.encoder[:2](x)\n",
    "        encoding_time = time.time() - t0\n",
    "\n",
    "        t0 = time.time()\n",
    "        pred = model(x)\n",
    "        model_time = time.time() - t0\n",
    "\n",
    "        t0 = time.time()\n",
    "        pred = codec.decoder(pred)\n",
    "        pred = codec.wavelet_synthesis(pred, codec.J)\n",
    "        pred = codec.clamp(pred) + 0.5\n",
    "        decoding_time = time.time()-t0\n",
    "\n",
    "        t0 = time.time()\n",
    "        pred = Resize(size=(img.height, img.width))(pred)\n",
    "        pred = pred.clamp(0,1)\n",
    "        post_time = time.time() - t0        \n",
    "\n",
    "        colorized = ToPILImage()(pred[0])\n",
    "        buff = io.BytesIO()\n",
    "        colorized.save(buff, format='WEBP', lossless=True)\n",
    "        colorized_bytes = buff.getbuffer()\n",
    "        \n",
    "        PSNR = psnr(pred,y)\n",
    "        MSSIM = multi_scale_ssim(pred,y)\n",
    "        LPIPS_dB = -10*np.log10(lpips_loss(pred, y).item())\n",
    "        DISTS_dB = -10*np.log10(dists_loss(pred, y).item())\n",
    "\n",
    "        gpu_mem = gpu_mem = torch.cuda.memory_reserved(0)/1e6 - gpu_mem_baseline\n",
    "        return {\n",
    "            'colorized': colorized_bytes,\n",
    "            'encoding_time': encoding_time,\n",
    "            'time': model_time + post_time,\n",
    "            'decoding_time': decoding_time,\n",
    "            'gpu_mem': gpu_mem,\n",
    "            'PSNR': PSNR,\n",
    "            'MSSIM': MSSIM,\n",
    "            'LPIPS_dB': LPIPS_dB,\n",
    "            'DISTS_dB': DISTS_dB,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba398d76-47a2-4eb8-9c5f-56031d1172fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b85c3ad9e9442e981bfe4f02958eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpu = dataset_valid.map(colorize_gpu)\n",
    "gpu = gpu.cast_column('colorized',HFImage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb6f87aa-5bd8-4d1e-96b9-7523d8324de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize_cpu(sample):\n",
    "    with torch.no_grad():\n",
    "        img = sample['image'].convert(\"RGB\")\n",
    "        y = PILToTensor()(img)\n",
    "        x = Grayscale(num_output_channels=3)(valid_transform(img))\n",
    "        x = x.to(torch.float)\n",
    "        y = y.to(torch.float)\n",
    "        x = (x / 255) - 0.5\n",
    "        y = y / 255\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        x = x.unsqueeze(0)\n",
    "        y = y.unsqueeze(0)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        x = codec.wavelet_analysis(x,codec.J)\n",
    "        x = codec.encoder[:2](x)\n",
    "        encoding_time = time.time() - t0\n",
    "\n",
    "        t0 = time.time()\n",
    "        pred = model(x)\n",
    "        model_time = time.time() - t0\n",
    "\n",
    "        t0 = time.time()\n",
    "        pred = codec.decoder(pred)\n",
    "        pred = codec.wavelet_synthesis(pred, codec.J)\n",
    "        pred = codec.clamp(pred) + 0.5\n",
    "        decoding_time = time.time()-t0\n",
    "\n",
    "        t0 = time.time()\n",
    "        pred = Resize(size=(img.height, img.width))(pred)\n",
    "        pred = pred.clamp(0,1)\n",
    "        post_time = time.time() - t0\n",
    "        \n",
    "        return {\n",
    "            'encoding_time': encoding_time,\n",
    "            'time': model_time + post_time,\n",
    "            'decoding_time': decoding_time,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66abc20d-95c1-434b-94fa-28e7fdafcf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd72504f6fea4db1a0f9473dcface7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model = model.to(device)\n",
    "codec = codec.to(device)\n",
    "cpu = dataset_valid.map(colorize_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61ba80a9-20ed-408a-9283-c738ef6aed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = gpu.add_column('cpu_time',cpu['time'])\n",
    "combined = combined.add_column('cpu_encoding_time',cpu['encoding_time'])\n",
    "combined = combined.add_column('cpu_decoding_time',cpu['decoding_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e44fc21-a116-485b-a510-f940b5c54f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80234e406d1640e8a306a060eeaeb529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caaa3c28b6454501b5dc4b8c009b4016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03e4654cae640578a6df977112181fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb392329e774be9a95ab83c9c58ffb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81dd5df5a0474b80bb862a240404b172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/danjacobellis/LSDIR_colorize_walloc_x16_512_p32/commit/2c61ca71b40868d5364b2c72bdc89ea6100d1c8e', commit_message='Upload dataset', commit_description='', oid='2c61ca71b40868d5364b2c72bdc89ea6100d1c8e', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.push_to_hub(\"danjacobellis/LSDIR_colorize_walloc_x16_512_p32\", split='validation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
